{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Advanced LLM Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "from langsmith.evaluation import EvaluationResult, RunEvaluator\n",
    "from langchain.evaluation.schema import LLMEvalChain, StringEvaluator\n",
    "from typing import Any, Optional\n",
    "import re\n",
    "from typing import Any, Optional\n",
    "\n",
    "from langchain.callbacks.manager import Callbacks\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.evaluation.schema import LLMEvalChain, StringEvaluator\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import RUN_KEY\n",
    "from langsmith.evaluation import EvaluationResult, RunEvaluator\n",
    "from langsmith.schemas import Example, Run\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(open(\"./config.yml\"))\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = config[\"OPENAI_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = config[\"LANGCHAIN_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_HUB_API_KEY\"] = config[\"LANGCHAIN_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = str(config[\"LANGCHAIN_TRACING_V2\"]).lower()\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = config[\"LANGCHAIN_ENDPOINT\"]\n",
    "os.environ[\"LANGCHAIN_HUB_API_URL\"] = config[\"LANGCHAIN_HUB_API_URL\"]\n",
    "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = str(config[\"LANGCHAIN_WANDB_TRACING\"]).lower()\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = config[\"LANGCHAIN_PROJECT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Creating a Simple Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_msg = \"\"\"[INST]You are a helpful assistant that translates English to French. Your task is to translate as a fluent speaker of both languages.\n",
    "Translate this sentence from English to French: {sentence}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "template = ChatPromptTemplate.from_template(sys_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator_chain = LLMChain(llm=OpenAI(), prompt=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'Hello, how are you?', 'text': '\\nBonjour, comment vas-tu ?'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator_chain.invoke({\"sentence\": \"Hello, how are you?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a simple translation evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt curation is 50% of success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE_CONSISTENCY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"result\"],\n",
    "    template=\"\"\"You are a highly skilled linguist tasked with evaluating the translation quality between a source text and its corresponding translated text. Your objective is to determine the accuracy, fluency, and appropriateness of the translation.\n",
    "\n",
    "To assess translation quality, carefully consider the following factors:\n",
    "- Accuracy: Analyze the translated text to ensure it accurately conveys the meaning and intent of the source text. Check for mistranslations, omissions, or additions that alter the original message.\n",
    "- Fluency: Evaluate the fluency and naturalness of the translated text in the target language. Consider grammar, syntax, idiomatic expressions, and readability. The translation should sound natural to native speakers of the target language.\n",
    "- Appropriateness: Assess the appropriateness of the translation in terms of tone, style, and cultural context. The translation should maintain the same register and adapt cultural references as needed for the target audience.\n",
    "\n",
    "Based on your analysis, provide a translation quality score from 0 to 10, where:\n",
    "0 - Extremely poor translation with significant errors and lack of fluency.\n",
    "2 - Poor translation with many inaccuracies and low fluency.\n",
    "4 - Below average translation with some inaccuracies and moderate fluency issues.\n",
    "6 - Acceptable translation with minor inaccuracies and good overall fluency.\n",
    "8 - High-quality translation with very few errors and excellent fluency.\n",
    "10 - Exceptional translation that accurately captures the source text's meaning, reads naturally in the target language, and maintains appropriate tone and style.\n",
    "\n",
    "In addition to the score, provide a detailed explanation of your assessment. Highlight specific aspects that support your evaluation, such as accuracy issues, fluency observations, and appropriateness considerations.\n",
    "\n",
    "Source Text: {query}\n",
    "Translated Text: {result}\n",
    "\n",
    "Strictly adhere to the following format in your response:\n",
    "SCORE: [0-10]\n",
    "REASONING: <Your brief one line reasoning explanation here. No newlines or line breaks>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function to parse the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_string_eval_output(text: str) -> dict:\n",
    "    score_pattern = r\"SCORE: (\\d+)\"\n",
    "    reasoning_pattern = r\"REASONING: (.*)\"\n",
    "\n",
    "    score_match = re.search(score_pattern, text)\n",
    "    reasoning_match = re.search(reasoning_pattern, text)\n",
    "\n",
    "    score = int(score_match.group(1)) if score_match else None\n",
    "    reasoning = reasoning_match.group(1).strip() if reasoning_match else None\n",
    "\n",
    "    return {\"score\": score, \"reasoning\": reasoning}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEvalChain(LLMChain, StringEvaluator, LLMEvalChain):\n",
    "    @property\n",
    "    def requires_reference(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(cls, llm: OpenAI, prompt: PromptTemplate, **kwargs: Any) -> \"BaseEvalChain\":\n",
    "        return cls(llm=llm, prompt=prompt, **kwargs)\n",
    "\n",
    "    def _prepare_output(self, result: dict) -> dict:\n",
    "        # parsing the output to extract the score and reasoning \n",
    "        parsed_result = _parse_string_eval_output(result[self.output_key])\n",
    "        \n",
    "        if RUN_KEY in result:\n",
    "            parsed_result[RUN_KEY] = result[RUN_KEY]\n",
    "        return parsed_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageConsistencyEvalChain(BaseEvalChain):\n",
    "    @property\n",
    "    def evaluation_name(self) -> str:\n",
    "        return \"Language Consistency\"\n",
    "\n",
    "    def _evaluate_strings(self, *, prediction: str, reference: Optional[str] = None, input: Optional[str] = None, callbacks: Callbacks = None, include_run_info: bool = False, **kwargs: Any) -> dict:\n",
    "        result = self({\"query\": input, \"result\": prediction}, callbacks=callbacks, include_run_info=include_run_info)\n",
    "        return self._prepare_output(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap it into an evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEvaluator(RunEvaluator):\n",
    "    def __init__(self, llm: Optional[OpenAI] = None, prompt: Optional[PromptTemplate] = None, eval_chain_class: Optional[BaseEvalChain] = None, k: int = 5):\n",
    "        self.llm = llm or OpenAI()\n",
    "        self.prompt = prompt\n",
    "        self.eval_chain_class = eval_chain_class\n",
    "        self.evaluator = self.eval_chain_class.from_llm(self.llm, prompt=self.prompt)\n",
    "        self.k = k\n",
    "\n",
    "    async def _evaluate_run_async(self, run: Run, example: Optional[Example] = None) -> dict:\n",
    "        if run.outputs is None:\n",
    "            raise ValueError(\"Run outputs cannot be None\")\n",
    "\n",
    "        response = run.outputs.get(\"text\")\n",
    "\n",
    "        if response is None:\n",
    "            raise ValueError(\"Run outputs must contain 'response' key\")\n",
    "\n",
    "        evaluation = await asyncio.to_thread(self.evaluator._evaluate_strings,\n",
    "                                             prediction=response,\n",
    "                                             input=example.inputs[\"question\"] if example else None,\n",
    "                                             context=run.outputs.get(\"context\"))\n",
    "\n",
    "        return evaluation\n",
    "\n",
    "    async def evaluate_run_async(self, run: Run, example: Optional[Example] = None) -> EvaluationResult:\n",
    "        tasks = []\n",
    "        for _ in range(self.k):\n",
    "            task = asyncio.create_task(self._evaluate_run_async(run, example))\n",
    "            tasks.append(task)\n",
    "\n",
    "        evaluations = await asyncio.gather(*tasks)\n",
    "\n",
    "        scores = [eval[\"score\"] for eval in evaluations]\n",
    "        reasonings = [eval[\"reasoning\"] for eval in evaluations]\n",
    "\n",
    "        avg_score = sum(scores) / len(scores)\n",
    "        closest_reasoning_index = min(range(len(scores)), key=lambda i: abs(scores[i] - avg_score))\n",
    "        closest_reasoning = reasonings[closest_reasoning_index]\n",
    "\n",
    "        return EvaluationResult(\n",
    "            key=self.evaluator.evaluation_name.lower().replace(\" \", \"_\"),\n",
    "            score=avg_score,\n",
    "            comment=closest_reasoning,\n",
    "        )\n",
    "        \n",
    "    def evaluate_run(self, run: Run, example: Optional[Example] = None) -> EvaluationResult:\n",
    "            return asyncio.run(self.evaluate_run_async(run, example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use default evaluator inside the Langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageConsistencyEvaluator(BaseEvaluator):\n",
    "    def __init__(self, llm: Optional[OpenAI] = None, prompt: Optional[PromptTemplate] = None):\n",
    "        super().__init__(llm, prompt or LANGUAGE_CONSISTENCY_PROMPT, LanguageConsistencyEvalChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the test evaluation\n",
    "langconst_evaluator = LanguageConsistencyEvaluator(llm=eval_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = langconst_evaluator.evaluator.invoke({\"query\": \"How are you? You good?\", \"result\": \"Comment ça va? Tu vas good?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How are you? You good?',\n",
       " 'result': 'Comment ça va? Tu vas good?',\n",
       " 'text': 'SCORE: 6\\nREASONING: Translation maintains casual tone and overall meaning but mixes languages (\"Tu vas good?\"), affecting fluency and appropriateness in a purely French context.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 6\n",
      "REASONING: Translation maintains casual tone and overall meaning but mixes languages (\"Tu vas good?\"), affecting fluency and appropriateness in a purely French context.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 10\n",
      "REASONING: The translated text accurately conveys the meaning, maintains the informal tone, and reads naturally in French, perfectly matching the source text's intent and style.\n"
     ]
    }
   ],
   "source": [
    "result = langconst_evaluator.evaluator.invoke({\"query\": \"How are you? You good?\", \"result\": \"Comment ça va? Tu vas bien?\"})\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Creating a More Advanced Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_RELEVANCE_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\"],\n",
    "    template=\"\"\"You are an expert evaluator assessing the relevance of retrieved context to a given query. Your task is to carefully analyze the provided context and determine if it contains information pertinent to answering the query. \n",
    "\n",
    "Consider the following:\n",
    "- Does the context directly address the main topics or entities mentioned in the query? \n",
    "- Does the context provide background information or details that could help form a comprehensive answer?\n",
    "- Is the context free of irrelevant or tangential information that could lead to hallucinations?\n",
    "\n",
    "Provide a relevance score from 0 to 10, where:\n",
    "0 - Completely irrelevant \n",
    "2 - Mostly irrelevant with minor pertinent details\n",
    "4 - Somewhat relevant, but missing key information \n",
    "8 - Mostly relevant, covering main points but lacking some specifics\n",
    "10 - Highly relevant, containing all necessary information to comprehensively address the query\n",
    "\n",
    "Along with the score, provide a concise explanation justifying your assessment. Highlight specific aspects of the context that informed your decision.\n",
    "\n",
    "Query: {query}\n",
    "Context: {context}\n",
    "\n",
    "Strictly answer in the following format:\n",
    "SCORE: [0-10]\n",
    "REASONING: <Your brief one line reasoning explanation here. No newlines or line breaks!>\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextRelevanceEvalChain(BaseEvalChain):\n",
    "    @property\n",
    "    def evaluation_name(self) -> str:\n",
    "        return \"Context Relevance\"\n",
    "\n",
    "    def _evaluate_strings(self, *, prediction: Optional[str] = None, reference: Optional[str] = None, context: Optional[str] = None, input: Optional[str] = None, callbacks: Callbacks = None, include_run_info: bool = False, **kwargs: Any) -> dict:\n",
    "        result = self({\"query\": input, \"context\": context}, callbacks=callbacks, include_run_info=include_run_info)\n",
    "        return self._prepare_output(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextRelevanceEvaluator(BaseEvaluator):\n",
    "    def __init__(self, llm: Optional[OpenAI] = None, prompt: Optional[PromptTemplate] = None):\n",
    "        super().__init__(llm, prompt or CONTEXT_RELEVANCE_PROMPT, ContextRelevanceEvalChain)\n",
    "\n",
    "    def evaluate_run(self, run: Run, example: Optional[Example] = None) -> EvaluationResult:\n",
    "        if run.outputs is None:\n",
    "            raise ValueError(\"Run outputs cannot be None\")\n",
    "\n",
    "        context = run.outputs.get(\"context\")\n",
    "\n",
    "        if context is None:\n",
    "            raise ValueError(\"Run outputs must contain 'context' key\")\n",
    "\n",
    "        return super().evaluate_run(run, example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the test evaluation\n",
    "context_evaluator = ContextRelevanceEvaluator(llm=eval_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [\n",
    "    \"Bla\",\n",
    "    \"Bla bla\",\n",
    "    \"Bla bla bla\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0\n",
      "REASONING: The provided context consists only of placeholder text with no information relevant to any moon landing.\n"
     ]
    }
   ],
   "source": [
    "result = context_evaluator.evaluator.invoke({\"query\": \"When was a moon landing?\", \"context\": context})\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [\n",
    "    \"The first successful manned Moon landing was accomplished by the Apollo 11 mission of NASA, with astronauts Neil Armstrong and Buzz Aldrin landing on the Moon on July 20, 1969.\"\n",
    "    \"There have been a total of six manned U.S. landings\",\n",
    "    \"To make an apple juice you need to peel the apple and ...\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 8\n",
      "REASONING: The context directly addresses the query with the date of the first manned Moon landing but includes unrelated information about making apple juice.\n"
     ]
    }
   ],
   "source": [
    "result = context_evaluator.evaluator.invoke({\"query\": \"When was a moon landing?\", \"context\": context})\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Evaluators on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "\n",
    "# Create a dataset\n",
    "dataset_inputs = [\n",
    "    \"How are you?\",\n",
    "    \"Where is the nearest restaurant?\",\n",
    "    \"All in all, actions speak louder than words when you're burning the candle at both ends. \",\n",
    "]\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Test_FR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Translation test dataset\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in dataset_inputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure evaluation using off-the-shelf metrics\n",
    "evaluation_config = RunEvalConfig(\n",
    "    custom_evaluators=[\n",
    "        LanguageConsistencyEvaluator(llm=eval_llm),\n",
    "    ],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'test-1' at:\n",
      "https://smith.langchain.com/o/477ed791-8ad2-52b5-a09b-1799811c6988/datasets/770968e1-8519-4ea8-9183-4af571cff33e/compare?selectedSessions=a15e919e-399a-4117-a525-de3c23335894\n",
      "\n",
      "View all tests for Dataset Test_FR at:\n",
      "https://smith.langchain.com/o/477ed791-8ad2-52b5-a09b-1799811c6988/datasets/770968e1-8519-4ea8-9183-4af571cff33e\n",
      "[------------------------------------------------->] 3/3"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'project_name': 'test-1',\n",
       " 'results': {'2aef3337-04c4-4292-a4aa-21e0796a1c8f': {'input': {'question': 'How are you?'},\n",
       "   'feedback': [EvaluationResult(key='language_consistency', score=10.0, value=None, comment='The translation \"Comment vas-tu?\" accurately conveys the meaning, is fluent and natural in French, and appropriately maintains the informal tone of the source text.', correction=None, evaluator_info={}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
       "   'execution_time': 0.752339,\n",
       "   'run_id': '679e1fc6-d071-4788-ab12-4d0a99554991',\n",
       "   'output': {'sentence': 'How are you?', 'text': '\\nComment vas-tu?'}},\n",
       "  '7c512c45-20c6-41ad-9e6d-31ffbedcc44b': {'input': {'question': 'Where is the nearest restaurant?'},\n",
       "   'feedback': [EvaluationResult(key='language_consistency', score=10.0, value=None, comment='The translation accurately conveys the meaning of the source text, uses natural and fluent French, and maintains the appropriate tone and style.', correction=None, evaluator_info={}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
       "   'execution_time': 0.447339,\n",
       "   'run_id': '14a28087-46ba-4811-881c-6873f81a5ad9',\n",
       "   'output': {'sentence': 'Where is the nearest restaurant?',\n",
       "    'text': '\\nOù se trouve le restaurant le plus proche?'}},\n",
       "  '328d6399-b385-4942-a757-2c79c8e8a17f': {'input': {'question': \"All in all, actions speak louder than words when you're burning the candle at both ends. \"},\n",
       "   'feedback': [EvaluationResult(key='language_consistency', score=8.8, value=None, comment='The translation accurately captures the meaning and idiomatic expression of the source text, maintaining fluency and appropriateness in French, with only minor room for subjective improvement in style or tone.', correction=None, evaluator_info={}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
       "   'execution_time': 0.617001,\n",
       "   'run_id': '9cfc3784-889c-403d-8daf-cec685b8a46d',\n",
       "   'output': {'sentence': \"All in all, actions speak louder than words when you're burning the candle at both ends. \",\n",
       "    'text': \"\\nDans l'ensemble, les actions parlent plus fort que les mots lorsque vous brûlez la chandelle par les deux bouts.\"}}},\n",
       " 'aggregate_metrics': None}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=translator_chain,\n",
    "    client=client,\n",
    "    evaluation=evaluation_config,\n",
    "    project_name=\"test-1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'project_name': 'test-1',\n",
    " 'results': {'2aef3337-04c4-4292-a4aa-21e0796a1c8f': {'input': {'question': 'How are you?'},\n",
    "   'feedback': [EvaluationResult(key='language_consistency', score=10.0, value=None, comment='The translation \"Comment vas-tu?\" accurately conveys the meaning, is fluent and natural in French, and appropriately maintains the informal tone of the source text.', correction=None, evaluator_info={}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
    "   'execution_time': 0.752339,\n",
    "   'run_id': '679e1fc6-d071-4788-ab12-4d0a99554991',\n",
    "   'output': {'sentence': 'How are you?', 'text': '\\nComment vas-tu?'}},\n",
    "  '7c512c45-20c6-41ad-9e6d-31ffbedcc44b': {'input': {'question': 'Where is the nearest restaurant?'},\n",
    "   'feedback': [EvaluationResult(key='language_consistency', score=10.0, value=None, comment='The translation accurately conveys the meaning of the source text, uses natural and fluent French, and maintains the appropriate tone and style.', correction=None, evaluator_info={}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
    "   'execution_time': 0.447339,\n",
    "   'run_id': '14a28087-46ba-4811-881c-6873f81a5ad9',\n",
    "...\n",
    "   'execution_time': 0.617001,\n",
    "   'run_id': '9cfc3784-889c-403d-8daf-cec685b8a46d',\n",
    "   'output': {'sentence': \"All in all, actions speak louder than words when you're burning the candle at both ends. \",\n",
    "    'text': \"\\nDans l'ensemble, les actions parlent plus fort que les mots lorsque vous brûlez la chandelle par les deux bouts.\"}}},\n",
    " 'aggregate_metrics': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cryptorag",
   "language": "python",
   "name": "cryptorag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
